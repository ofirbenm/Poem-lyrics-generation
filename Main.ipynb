{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ccd470b3-5a5b-4824-b8b4-b9d557c8b7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install tensorboardX\n",
    "# pip install jupyter_tensorboard\n",
    "pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfa612b0-3ea0-40a6-9948-3c7978b01b6c",
   "metadata": {
    "id": "hNfpZsly7-pu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Preprocessing.ipynb\n",
      "importing Jupyter notebook from DataLoader_class.ipynb\n",
      "importing Jupyter notebook from ass3_deep.ipynb\n",
      "PyTorch Version: 1.13.1+cu117\n",
      "\n",
      "Python 3.7.11 (default, Jul 27 2021, 14:32:16) \n",
      "[GCC 7.5.0]\n",
      "Pandas 1.3.5\n",
      "Scikit-Learn 1.0.2\n",
      "GPU is available\n",
      "importing Jupyter notebook from RNN.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from torchtext.data import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import gensim.downloader\n",
    "from time import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# ipynb\n",
    "from Preprocessing import *\n",
    "from DataLoader_class import *\n",
    "from ass3_deep import *\n",
    "from RNN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18f8b684-05f7-4657-9ca2-f10248e1ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3800d81a-47c9-4da1-b547-5ebe733f8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = 'ass3_file'\n",
    "MIDI_PATH =  os.path.join(DATA_PATH,'midi_files/')\n",
    "TRAIN_PATH =  os.path.join(DATA_PATH,'lyrics_train_set.csv')\n",
    "TEST_PATH =  os.path.join(DATA_PATH,'lyrics_test_set.csv')\n",
    "PICK_PATH = os.path.join(DATA_PATH,'pickle_file')\n",
    "TRAIN_PKL_PATH = os.path.join(PICK_PATH,'train_df.pkl')\n",
    "TEST_PKL_PATH = os.path.join(PICK_PATH,'test_df.pkl')\n",
    "PATH_LOG = os.path.join(DATA_PATH,'logs')\n",
    "WORD2VEC_name = 'glove-wiki-gigaword-300'\n",
    "word2vec = gensim.downloader.load(WORD2VEC_name)\n",
    "\n",
    "# Variables\n",
    "VECTOR_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee5efd89-790b-4b23-80ef-66e2a5c754ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open pikcle files\n",
    "train_df = pd.read_pickle(TRAIN_PKL_PATH)\n",
    "test_df = pd.read_pickle(TEST_PKL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dcbf6f8-f2ea-4090-ba1f-7ef4984537b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:02,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    if not os.path.exists(TRAIN_PKL_PATH):\n",
    "        # Rename to lower case\n",
    "        for file in os.listdir(MIDI_PATH):\n",
    "            os.rename(MIDI_PATH + file, MIDI_PATH + file.lower())\n",
    "                 \n",
    "        # Dataframes\n",
    "        train_df = (pd.read_csv(TRAIN_PATH, header = None)\n",
    "                .rename(columns={0:'artist',1:'song',2:'lyrics'})\n",
    "                .drop(columns=[3,4,5,6], axis=1))\n",
    "\n",
    "        test_df = (pd.read_csv(TEST_PATH, header = None)\n",
    "                .rename(columns={0:'artist',1:'song',2:'lyrics'}))\n",
    "        preprocess(train_df, test_df, word2vec)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5b8c200-2814-4d9a-a892-cbc7f38bdf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(train_df['tokens'])+list(test_df['tokens'])\n",
    "tokens_lst = list(set([word for lst in tokens for word in lst]))\n",
    "total_words = len(tokens_lst)\n",
    "index2word = {i:tokens_lst[i] for i in range(len(tokens_lst))}\n",
    "word2index = {tokens_lst[i]:i for i in range(len(tokens_lst))}\n",
    "\n",
    "# Convert list of tokens to list of indexes\n",
    "train_df['tokens'] = train_df['tokens'].apply(lambda x: text2index(x, word2index))\n",
    "test_df['tokens'] = test_df['tokens'].apply(lambda x: text2index(x, word2index))\n",
    "\n",
    "word2vec_matrix = get_word2vec_matrix(total_words=len(tokens_lst),\n",
    "                                      index2word=index2word,\n",
    "                                      word2vec=word2vec,\n",
    "                                      vector_size=VECTOR_SIZE)\n",
    "\n",
    "all_tokens = [token for sublist in list(train_df['tokens']) for token in sublist]\n",
    "tf_tokens = Counter(all_tokens)\n",
    "#aplly log on each freq\n",
    "tf_tokens = {key:np.log2(val+1) for key, val in tf_tokens.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde445c8-d725-4e03-aa9b-9c5f53c89429",
   "metadata": {},
   "source": [
    "## Parmeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "37ce69ca-e903-4a55-a471-0054830162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 50\n",
    "# params = {'learning_rate' : [0.001, 0.01], 'batch_size' : [32, 64], 'num_layers': [1,2], 'units' : [256,512], 'seq_length':[1,5,9]}\n",
    "\n",
    "# best params\n",
    "params = {'learning_rate' : [0.001], 'batch_size' : [32], 'num_layers': [2], 'units' : [512], 'seq_length':[9]} \n",
    "\n",
    "# Create a SummaryWriter object\n",
    "# log_writer = SummaryWriter(log_dir=PATH_LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fc93d2-186f-48f3-b2f3-8f5bbbca4af5",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03810bf8-8ca7-4f3b-af2d-924f15be2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(parameters, DataLoader_train, size, model, optimizer, loss_fun):\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        # print(f'Epoch : {epoch}')\n",
    "        current_loss = 0\n",
    "        for step, (X, y, tf, features) in enumerate(DataLoader_train):\n",
    "            input_sequence, output_sequence, tf, features = X.to(device), y.to(device), tf.to(device), features.to(device)\n",
    "            pred = model(input_sequence, features)\n",
    "            # loss\n",
    "            loss = loss_fun(pred, output_sequence, tf)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current_loss += loss.item()\n",
    "            \n",
    "            # Log scalars\n",
    "            log_writer.add_scalar('train/loss', loss.item(), epoch * len(DataLoader_train) + step)\n",
    "            \n",
    "        if epoch == 0:\n",
    "            first_loss = current_loss \n",
    "        if epoch == NUM_EPOCH - 1:\n",
    "            final_loss = current_loss \n",
    "\n",
    "\n",
    "    final_loss = final_loss/size \n",
    "    print(f'batch_size: {parameters[\"batch_size\"]}, num_layers: {parameters[\"num_layers\"]}, units: {parameters[\"units\"]}, lr: {parameters[\"learning_rate\"]}, seq_len: {parameters[\"seq_length\"]}')\n",
    "    print(f'first_loss: {first_loss/size}, final_Loss: {final_loss}')\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    return model, final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02089491-9004-4a99-b1a8-0ea6b4234171",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c400aa54-7884-46d3-b3be-faec77083782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:20<00:00, 200.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32, num_layers: 2, units: 512, lr: 0.001, seq_len: 9\n",
      "first_loss: 0.03486593927882144, final_Loss: 0.022081862190279408\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for parameters in tqdm(ParameterGrid(params)):\n",
    "\n",
    "    # Create sequences\n",
    "    encoded_lyrics_list = list(train_df['tokens'])\n",
    "    features_list = list(train_df['feature_method1'])\n",
    "    \n",
    "    input_sequences, word2vec_next_words, tokens_tf_array, input_features = create_sequences(encoded_lyrics_list, features_list, total_words, parameters['seq_length'], word2vec_matrix , VECTOR_SIZE,tf_tokens)\n",
    "    X_train, y_train = input_sequences, word2vec_next_words\n",
    "    \n",
    "    # Train DataLoader\n",
    "    SongDataset_train = SongDataset(X_train,  y_train, tokens_tf_array, input_features)\n",
    "    DataLoader_train = DataLoader(SongDataset_train, batch_size=parameters['batch_size'], shuffle=True)\n",
    "    size = len(DataLoader_train)\n",
    "\n",
    "    # Initialize the LSTM Network\n",
    "    model = LSTMLyrics(total_words=total_words, vector_size=VECTOR_SIZE, word2vec_matrix=word2vec_matrix,num_layers = parameters['num_layers'], units=parameters['units'], features_size=2).to(device)\n",
    "\n",
    "    # Adam Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=parameters['learning_rate'])\n",
    "\n",
    "    # Loss - cross entropy loss\n",
    "    cos = Custom_L1_Loss()    \n",
    "    \n",
    "    start = time()\n",
    "    model.train()\n",
    "    model, loss = train(parameters, DataLoader_train, size, model, optimizer, cos)\n",
    "    train_time = time() - start\n",
    "    \n",
    "    #result\n",
    "    temp_result = list(parameters.values())\n",
    "    temp_result += [round(train_time,3),loss]\n",
    "    results += [temp_result]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8eef3698-b53c-4ce5-8be5-f0c9a9c63374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>feature_method1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1791, 1348, 2157, 1681, 1874, 1348, 6395, 670...</td>\n",
       "      <td>[[3, 3], [3, 4], [2, 4], [3, 12], [2, 11], [3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[4456, 4073, 5699, 4272, 964, 2209, 321, 1263,...</td>\n",
       "      <td>[[2, 0], [1, 0], [1, 0], [1, 0], [2, 0], [1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[5139, 6554, 2531, 2984, 4642, 1350, 6162, 407...</td>\n",
       "      <td>[[2, 11], [1, 6], [1, 6], [1, 5], [2, 9], [1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[72, 285, 3078, 6744, 2562, 2650, 4073, 6342, ...</td>\n",
       "      <td>[[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[147, 2056, 2723, 5413, 2477, 6147, 1801, 2054...</td>\n",
       "      <td>[[3, 0], [2, 30], [2, 31], [3, 41], [2, 39], [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [1791, 1348, 2157, 1681, 1874, 1348, 6395, 670...   \n",
       "1  [4456, 4073, 5699, 4272, 964, 2209, 321, 1263,...   \n",
       "2  [5139, 6554, 2531, 2984, 4642, 1350, 6162, 407...   \n",
       "3  [72, 285, 3078, 6744, 2562, 2650, 4073, 6342, ...   \n",
       "4  [147, 2056, 2723, 5413, 2477, 6147, 1801, 2054...   \n",
       "\n",
       "                                     feature_method1  \n",
       "0  [[3, 3], [3, 4], [2, 4], [3, 12], [2, 11], [3,...  \n",
       "1  [[2, 0], [1, 0], [1, 0], [1, 0], [2, 0], [1, 0...  \n",
       "2  [[2, 11], [1, 6], [1, 6], [1, 5], [2, 9], [1, ...  \n",
       "3  [[1, 0], [1, 0], [1, 0], [1, 0], [1, 0], [1, 0...  \n",
       "4  [[3, 0], [2, 30], [2, 31], [3, 41], [2, 39], [...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a98173-8b46-4fe5-9026-9d2763c40bcd",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c4a0ab5-7fd7-4f9e-8a55-12f3b71acffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(encoded_lyrics_list, features_list, total_words, seq_length, word2vec, vector_size, tokens_tf,test = False,):\n",
    "    \"\"\"\n",
    "    This function creates sequences from the lyrics\n",
    "    :param encoded_lyrics_list: A list representing all the songs in the dataset (615 songs). Each cell contains a list\n",
    "    of ints, where each int corresponds to the lyrics in that song. \"I'm a barbie girl\" --> [23, 52, 189, 792] etc.\n",
    "    :param total_words: Number of words in our word2vec dictionary.\n",
    "    :param seq_length: Number of words predating the word to be predicted.\n",
    "    :return: (1) A numpy array containing all the sequences seen, concatenated.\n",
    "             (2) A 2d numpy array where each row represents a word and the columns are the possible words in the\n",
    "             vocabulary. There is a '1' in the corresponding word (e.g, word number '20,392' in the dataset is word\n",
    "              number '39' in the vocab.\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "    next_words = []\n",
    "    next_tf = []\n",
    "    lst_features = []\n",
    "    for idx, song_sequence in enumerate(encoded_lyrics_list):  # iterate over songs\n",
    "        feature_sequence = features_list[idx]\n",
    "        for i in range(seq_length, len(song_sequence), seq_length):  # iterate from minimal sequence length (number of words) to\n",
    "            start_index = i - seq_length  # number of words in the song\n",
    "            end_index = i\n",
    "            # Slice the list into the desired sequence length\n",
    "            sequence = song_sequence[start_index:end_index]\n",
    "            features = feature_sequence[start_index:end_index]\n",
    "            next_word = song_sequence[start_index+1:end_index+1]\n",
    "            \n",
    "            # append to lists\n",
    "            input_sequences.append(sequence)\n",
    "            lst_features.append(features)\n",
    "            next_words.append(next_word)\n",
    "            if test : \n",
    "                break\n",
    "            next_tf.append([tokens_tf[tf] for tf in next_word])\n",
    "            \n",
    "    input_sequences = np.array(input_sequences)\n",
    "    input_features = np.array(lst_features)\n",
    "    word2vec_next_word = convert_to_word2vec(word2vec, next_words, vector_size, seq_length)\n",
    "    return input_sequences, word2vec_next_word, np.array(next_tf), input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7bb826f3-39ce-4b7e-aaf1-2748abec3a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_lyrics_list = list(test_df['tokens'])\n",
    "words = [word[1] for word in encoded_lyrics_list]\n",
    "index2word[4073]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba8adf4d-267e-48e7-bdcb-5cfecfe5ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: eyes\n",
      "------------------------\n",
      "eyes however know not can take go so only even because so , way going just off not what so come why others both only would what can just let we because think when because but though although just you go sure even time if both do should take get just get out even both but\n",
      "start: search\n",
      "------------------------\n",
      "search even what not but they know sure well way be what both you would get do so did come now only but know same when just going ? put it come it going so all if just know up because also so get younger only not well feel even so you thing if make really me thing think but so though this think . , though not going you to think ? but however think things when well not way as although your go feelings you because what well i so think come well but\n",
      "start: fear\n",
      "------------------------\n",
      "fear even think its if going i instead need it but others if so things so could but been so because one in only one something get everyone going when then if what when just so everyone those if so will everyone be same know others come everyone back well not just what ought know able well them just even it should come even going even before up although . now they when looked maybe now they time\n",
      "start: hi\n",
      "------------------------\n",
      "hi ! our they want not give come only in so ? even well anything this maybe which all way same also this everybody even it but just this does but its why not even what should things so that just really that wanted if but ? husband when know really why do others they ? nothing also so say kinda we bit what it what think take even get if they if might want . try but only guess yells anything they we do i and what go think because time in all\n",
      "start: small\n",
      "------------------------\n",
      "small now this but going just again something it one get well just same take . only but go somebody this know everybody as what make really they want why what as my ? know . know same know come we not if you what even you sure something we though we same even both one even only it\n"
     ]
    }
   ],
   "source": [
    "seq_length =  1\n",
    "length_lst = [55,95,77,93,58]\n",
    "# Create sequences\n",
    "encoded_lyrics_list = list(test_df['tokens'])\n",
    "wordss = [word[2] for word in encoded_lyrics_list]\n",
    "\n",
    "features_list = list(test_df['feature_method1'])[0]\n",
    "\n",
    "input_sequences, word2vec_next_words, _ , input_features = create_sequences(encoded_lyrics_list, features_list, total_words, seq_length, word2vec_matrix , VECTOR_SIZE,None, True)\n",
    "X_test, y_test = input_sequences, word2vec_next_words\n",
    "SongDataset_test = SongDataset(X_test,  y_test, None, input_features)\n",
    "\n",
    "DataLoader_test = DataLoader(SongDataset_test, batch_size= 1, shuffle=False)\n",
    "\n",
    "for  idx, (X,y, _, _) in enumerate(DataLoader_test):\n",
    "    # Begin of the song\n",
    "    z = wordss[idx]\n",
    "    print(f'start: {index2word[z]}')\n",
    "    # print(f'start: {[index2word[x] for x in X.to(\"cpu\").detach().numpy()[0]]}')\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "    # List of tokens for adding every step the predicted next token in the sequence\n",
    "    x_cpu = [z]\n",
    "            \n",
    "    WORDS_NUM = length_lst[idx]\n",
    "    for i in range(WORDS_NUM):\n",
    "        # Next sequence for the model\n",
    "        if len(x_cpu) <= 5:\n",
    "            seq = x_cpu[:len(x_cpu)]\n",
    "            features_seq = features_list[:len(x_cpu)]\n",
    "        else:\n",
    "            seq = x_cpu[i-5:i]\n",
    "            features_seq = features_list[i-5:i]\n",
    "\n",
    "        # reshape with batch_size=1\n",
    "        reshape_seq = np.array(seq).reshape((1,len(seq)))\n",
    "        reshape_features = np.array(features_seq).reshape((1,len(seq),2))\n",
    "\n",
    "        \n",
    "        input_sequence = torch.tensor(reshape_seq, dtype=torch.long).to(device)\n",
    "        input_features = torch.tensor(reshape_features, dtype=torch.long).to(device)\n",
    "        \n",
    "        vector_pred = model(input_sequence, input_features).to('cpu').detach().numpy()[0]\n",
    "        \n",
    "        top_n = 20\n",
    "                \n",
    "        # Sampling\n",
    "        closest_words = word2vec.most_similar(vector_pred[-1],topn=top_n)\n",
    "        closest_words = [tup for tup in closest_words if tup[0] in word2index]\n",
    "        words = np.array([word for word, similarity in closest_words])\n",
    "        similarities = np.array([similarity for word, similarity in closest_words])\n",
    "        weights = softmax_stable(similarities)\n",
    "        # Sample\n",
    "        while True:\n",
    "            sample = np.random.choice(words, p=weights)\n",
    "            if sample != index2word[x_cpu[-1]]:\n",
    "                break\n",
    "        \n",
    "        x_cpu.append(word2index[sample])\n",
    "    \n",
    "    words = [index2word[x] for x in x_cpu]\n",
    "    print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "18618197-deba-430b-b428-897f18c0c8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hiya barbie hi ken ! do you want to go for a ride ? sure ken jump in im a barbie girl in a barbie world life in plastic its fantastic you can brush my hair undress me everywhere imagination life is your creation come on barbie lets go party ! im a barbie girl in a barbie world life in plastic its fantastic you can brush my hair undress me everywhere imagination life is your creation im a blond bimbo girl in a fantasy world dress me up make it tight im your dolly youre my doll rocknroll feel the glamor in pink kiss me here touch me there hanky panky you can touch you can play if you say im always yours im a barbie girl in a barbie world life in plastic its fantastic you can brush my hair undress me everywhere imagination life is your creation come on barbie lets go party ! ( ah ah ah yeah ) come on barbie lets go party ! ( oh oh ) come on barbie lets go party ! ( ah ah ah yeah ) come on barbie lets go party ! ( oh oh ) make me walk make me talk do whatever you please i can act like a star i can beg on my knees come jump in bimbo friend let us do it again hit the town fool around lets go party you can touch you can play if you say im always yours you can touch you can play if you say im always yours come on barbie lets go party ! ( ah ah ah yeah ) come on barbie lets go party ! ( oh oh ) come on barbie lets go party ! ( ah ah ah yeah ) come on barbie lets go party ! ( oh oh ) im a barbie girl in a barbie world life in plastic its fantastic you can brush my hair undress me everywhere imagination life is your creation im a barbie girl in a barbie world life in plastic its fantastic you can brush my hair undress me everywhere imagination life is your creation come on barbie lets go party ! ( ah ah ah yeah ) come on barbie lets go party ! ( oh oh ) come on barbie lets go party ! ( ah ah ah yeah ) come on barbie lets go party ! ( oh oh ) oh im having so much fun ! well barbie we are just getting started oh i love you ken'"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([index2word[x] for x in list(test_df['tokens'].iloc[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50c7772b-93a8-407d-b2d7-5dc704c6e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_stable(x):\n",
    "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751448ac-a793-4fd7-baa8-192cf48da23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lstm neural network\n",
    "# the output in create sequences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ass3_GPU",
   "language": "python",
   "name": "ass3_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
