{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pretty_midi\n",
    "from tqdm import tqdm\n",
    "from torchtext.data import get_tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "import sklearn as sk\n",
    "import zipfile\n",
    "import csv\n",
    "import string\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CeGpvjWv6PSz"
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'ass3_file'\n",
    "MIDI_PATH =  os.path.join(DATA_PATH,'midi_files/')\n",
    "TRAIN_PATH =  os.path.join(DATA_PATH,'lyrics_train_set.csv')\n",
    "TEST_PATH =  os.path.join(DATA_PATH,'lyrics_test_set.csv')\n",
    "PICK_PATH = os.path.join(DATA_PATH,'pickle_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yC58yhCoLP9T"
   },
   "outputs": [],
   "source": [
    "for file in os.listdir(MIDI_PATH):\n",
    "    os.rename(MIDI_PATH + file, MIDI_PATH + file.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GHCDcKWEVwv3"
   },
   "outputs": [],
   "source": [
    "train_df = (pd.read_csv(TRAIN_PATH, header = None)\n",
    "            .rename(columns={0:'artist',1:'song',2:'lyrics'})\n",
    "            .drop(columns=[3,4,5,6], axis=1))\n",
    "\n",
    "test_df = (pd.read_csv(TEST_PATH, header = None)\n",
    "            .rename(columns={0:'artist',1:'song',2:'lyrics'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_F6Dsl7FjM4"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-NECb6YO9Ngj"
   },
   "outputs": [],
   "source": [
    "def clean_lyrics(lyrics,word2vec):\n",
    "    lyrics = lyrics.replace('&', '')\n",
    "    lyrics = lyrics.replace('  ', ' ')\n",
    "    lyrics = lyrics.replace('\\'', '')\n",
    "    lyrics = lyrics.replace('--', ' ')\n",
    "    lyrics = lyrics.replace('[', '')\n",
    "    lyrics = lyrics.replace(']', '')\n",
    "    lyrics = lyrics.replace('-', ' ')\n",
    "    \n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "S5gWlsdvq0VM"
   },
   "outputs": [],
   "source": [
    "def all_preprocessing(df,word2vec):\n",
    "    \n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "    df['lyrics'] = df['lyrics'].apply(lambda lyrics: clean_lyrics(lyrics,word2vec))\n",
    "    df['tokens'] = df['lyrics'].apply(lambda text: tokenizer(text))\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [token for token in tokens if token in word2vec])\n",
    "\n",
    "\n",
    "    file_names = []\n",
    "    #change the file name \n",
    "    for index,row in df.iterrows():\n",
    "        file_names.append(row[0].replace(\" \",\"_\")+\"_-_\"+row[1].replace(\" \",\"_\")+\".mid\")\n",
    "\n",
    "    df['file_name'] = file_names\n",
    "    df['pretty_format'] = df['file_name'].apply(lambda file_name: convert_pretty_format(file_name))\n",
    "    df = df[df['pretty_format'].notna()]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGLLU1WrFuJ3"
   },
   "source": [
    "# Extract melodies features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2GNaZvHgRkrd"
   },
   "outputs": [],
   "source": [
    "def convert_pretty_format(file_name):\n",
    "    try:\n",
    "        file_path =  os.path.join(MIDI_PATH, file_name)\n",
    "        return pretty_midi.PrettyMIDI(file_path)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def extract_mellidies_feature_method1(df,feature_version=1):\n",
    "    all_sons_feature = []\n",
    "\n",
    "    for index,row in tqdm(df.iterrows()):\n",
    "        song_feature = []\n",
    "        midi_file = row['pretty_format']\n",
    "        num_words = len(row['tokens'])\n",
    "        midi_file.remove_invalid_notes()\n",
    "        mean_time_word = midi_file.get_end_time() / num_words  #calculate the mean time per word in every song\n",
    "\n",
    "        for index_word in range(num_words):\n",
    "            if feature_version == 1:\n",
    "                word_feature = get_instructor_beats(midi_file, index_word, mean_time_word)\n",
    "            else:\n",
    "                word_feature = extract_piano_feature(midi_file, index_word, mean_time_word, num_words)\n",
    "            song_feature.append(word_feature)\n",
    "\n",
    "        all_sons_feature.append(np.array(song_feature))\n",
    "\n",
    "    return all_sons_feature\n",
    "\n",
    "def get_instructor_beats(midi_file, idx_word,mean_time):\n",
    "    \"\"\"\n",
    "    Extract from each midi file - number of instructor, change beats - in eatch word time period.\n",
    "\n",
    "    :idx_word - the index of word: to calculate the time range of the word.\n",
    "    :mean_time - The mean time for the words in each song. \n",
    "    :return: Array that contain [numbe of beats, number of instructor]  \n",
    "    \"\"\"\n",
    "    beats, instructor = 0,0\n",
    "    start_time = idx_word * mean_time\n",
    "    end_time = start_time + mean_time\n",
    "\n",
    "    for beat in midi_file.get_beats():\n",
    "        if start_time <= beat and beat <= end_time:\n",
    "              beats +=1\n",
    "\n",
    "    for instrument in midi_file.instruments:\n",
    "        for note in instrument.notes:\n",
    "              if start_time <= note.start and note.end <= end_time:\n",
    "                    instructor+=1\n",
    "\n",
    "    return np.array([beats,instructor])\n",
    "\n",
    "def extract_piano_feature(midi_file, idx_word,mean_time,num_of_words):\n",
    "    piano_roll = midi_file.get_piano_roll()\n",
    "    notes_per_word = int(piano_roll.shape[1] / num_of_words) \n",
    "\n",
    "    start_index = idx_word * idx_word\n",
    "    end_index = start_index + idx_word\n",
    "\n",
    "    piano_for_lyric = piano_roll[:, start_index:end_index].transpose()\n",
    "    piano_sum = np.sum(piano_for_lyric, axis=0)\n",
    "\n",
    "    return piano_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2index(lst_tokens, word2index):\n",
    "    return [word2index[token] for token in lst_tokens]\n",
    "\n",
    "def create_sequences(encoded_lyrics_list, features_list, total_words, seq_length, word2vec, vector_size, tokens_tf,test = False,):\n",
    "    \"\"\"\n",
    "    This function creates sequences from the lyrics\n",
    "    :param encoded_lyrics_list: A list representing all the songs in the dataset (615 songs). Each cell contains a list\n",
    "    of ints, where each int corresponds to the lyrics in that song. \"I'm a barbie girl\" --> [23, 52, 189, 792] etc.\n",
    "    :param total_words: Number of words in our word2vec dictionary.\n",
    "    :param seq_length: Number of words predating the word to be predicted.\n",
    "    :return: (1) A numpy array containing all the sequences seen, concatenated.\n",
    "             (2) A 2d numpy array where each row represents a word and the columns are the possible words in the\n",
    "             vocabulary. There is a '1' in the corresponding word (e.g, word number '20,392' in the dataset is word\n",
    "              number '39' in the vocab.\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "    next_words = []\n",
    "    next_tf = []\n",
    "    lst_features = []\n",
    "    for idx, song_sequence in enumerate(encoded_lyrics_list):  # iterate over songs\n",
    "        feature_sequence = features_list[idx]\n",
    "        for i in range(seq_length, len(song_sequence), seq_length):  # iterate from minimal sequence length (number of words) to\n",
    "            start_index = i - seq_length  # number of words in the song\n",
    "            end_index = i\n",
    "            # Slice the list into the desired sequence length\n",
    "            sequence = song_sequence[start_index:end_index]\n",
    "            features = feature_sequence[start_index:end_index]\n",
    "            next_word = song_sequence[start_index+1:end_index+1]\n",
    "            \n",
    "            # append to lists\n",
    "            input_sequences.append(sequence)\n",
    "            lst_features.append(features)\n",
    "            next_words.append(next_word)\n",
    "            if test : \n",
    "                break\n",
    "            next_tf.append([tokens_tf[tf] for tf in next_word])\n",
    "            \n",
    "    input_sequences = np.array(input_sequences)\n",
    "    input_features = np.array(lst_features)\n",
    "    word2vec_next_word = convert_to_word2vec(word2vec, next_words, vector_size, seq_length)\n",
    "    return input_sequences, word2vec_next_word, np.array(next_tf), input_features\n",
    "\n",
    "def convert_to_one_hot_encoding(input_sequences, next_words, total_words):\n",
    "    \"\"\"\n",
    "    This function converts input to one hot encoding\n",
    "    \"\"\"\n",
    "    one_hot_encoding_next_words = np.zeros((len(input_sequences), total_words), dtype=np.int8)\n",
    "    for word_index, word in enumerate(next_words):\n",
    "        one_hot_encoding_next_words[word_index, word] = 1\n",
    "    return one_hot_encoding_next_words\n",
    "\n",
    "def convert_to_word2vec(word2vec, next_word, vector_size, seq_length):\n",
    "    word2vec_next_words = np.zeros((len(next_word), seq_length, vector_size), dtype=np.float32)\n",
    "    for idx, sequence in enumerate(next_word):\n",
    "        for idy, word in enumerate(sequence):\n",
    "            word2vec_next_words[idx, idy,]= word2vec[word]\n",
    "    return word2vec_next_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec(word2vec_path, pre_trained, vector_size, encoding='utf-8') -> dict:\n",
    "    \"\"\"\n",
    "    This function returns a dictionary that maps between word and a vector\n",
    "    :param word2vec_path: path for the pickle file\n",
    "    :param pre_trained: path for the pre-trained embedding file\n",
    "    :param vector_size: the vector size for each word\n",
    "    :param encoding: the encoding the the pre_trained file\n",
    "    :return: dictionary maps between a word and a vector\n",
    "    \"\"\"\n",
    "    # If the pickle file is already exists, read that file\n",
    "    word2vec = _read_pickle_if_exists(word2vec_path)\n",
    "    if word2vec is None:  # The pickle file is not exists.\n",
    "        with open(pre_trained, 'r', encoding=encoding) as f:  # Read a pre-trained word vectors.\n",
    "            list_of_lines = list(f)\n",
    "        word2vec = _iterate_over_glove_list(list_of_lines=list_of_lines, vector_size=vector_size)\n",
    "        _save_pickle(pickle_path=word2vec_path, content=word2vec)  # Save pickle for the next running                \n",
    "    return word2vec\n",
    "\n",
    "def get_word2vec_matrix(total_words, index2word, word2vec, vector_size):\n",
    "    \"\"\"\n",
    "    This function creates a matrix where the rows are the words and the columns represents the embedding vector.\n",
    "    We will use this matrix in the embedding layer\n",
    "    :param total_words: Number of words in our word2vec dictionary.\n",
    "    :param index2word: dictionary maps between index and word\n",
    "    :param word2vec: dictionary maps between a word and a vector\n",
    "    :param vector_size: the size of the embedding vector size\n",
    "    :return: embedding layer\n",
    "    \"\"\"\n",
    "    word2vec_matrix = np.zeros((total_words, vector_size))\n",
    "    for index_word, word in index2word.items():\n",
    "        if word not in word2vec:\n",
    "            print(f'Can not find the word \"{word}\" in the word2vec dictionary')\n",
    "            continue\n",
    "        else:\n",
    "            vec = word2vec[word]\n",
    "            word2vec_matrix[index_word] = vec\n",
    "    return word2vec_matrix\n",
    "\n",
    "def _iterate_over_glove_list(list_of_lines, vector_size):\n",
    "    \"\"\"\n",
    "    This function iterates over the glove list line by line and returns a word2vec dictionary\n",
    "    :param list_of_lines: List of glove lines\n",
    "    :param vector_size: the size of the embedding vector size\n",
    "    :return: dictionary maps between a word and a vector\n",
    "    \"\"\"\n",
    "    \n",
    "    word2vec = {}\n",
    "    punctuation = string.punctuation\n",
    "    for line in list_of_lines:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        if word in punctuation:\n",
    "            continue\n",
    "        vec = np.asarray(values[1:], \"float32\")\n",
    "        if len(vec) != vector_size:\n",
    "            raise Warning(f\"Vector size is different than {vector_size}\")\n",
    "        else:\n",
    "            word2vec[word] = vec\n",
    "    return word2vec\n",
    "\n",
    "\n",
    "def _save_pickle(pickle_path, content):\n",
    "    \"\"\"\n",
    "    This function saves a value to pickle file\n",
    "    :param pickle_path: path for the pickle file\n",
    "    :param content: the value you want to save\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(content, f)\n",
    "\n",
    "\n",
    "def _read_pickle_if_exists(pickle_path):\n",
    "    \"\"\"\n",
    "    This function reads a pickle file\n",
    "    :param pickle_path:path for the pickle file\n",
    "    :return: the saved value in the pickle file\n",
    "    \"\"\"\n",
    "    pickle_file = None\n",
    "    if os.path.exists(pickle_path):\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            pickle_file = pickle.load(f)\n",
    "    return pickle_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(train_df, test_df, word2vec):    \n",
    "    train_df = all_preprocessing(train_df, word2vec)\n",
    "    test_df = all_preprocessing(test_df, word2vec)\n",
    "\n",
    "    train_df['feature_method1'] = extract_mellidies_feature_method1(train_df)\n",
    "    # train_df['feature_method2'] = extract_mellidies_feature_method1(train_df, feature_version=2)\n",
    "    \n",
    "    test_df['feature_method1'] = extract_mellidies_feature_method1(test_df)\n",
    "    # test_df['feature_method2'] = extract_mellidies_feature_method1(test_df, feature_version=2)\n",
    "    \n",
    "    train_df = train_df.drop(['artist', 'song', 'lyrics', 'file_name', 'pretty_format'], axis = 1)\n",
    "    test_df = test_df.drop(['artist', 'song', 'lyrics', 'file_name', 'pretty_format'], axis = 1)\n",
    "\n",
    "    train_df.to_pickle(os.path.join(PICK_PATH,'train_df.pkl'))\n",
    "    test_df.to_pickle(os.path.join(PICK_PATH,'test_df.pkl'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ass3_GPU",
   "language": "python",
   "name": "ass3_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
